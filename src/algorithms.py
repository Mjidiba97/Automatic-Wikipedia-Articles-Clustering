import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Text processing
import wikipedia
import re
import nltk
from nltk.stem.snowball import SnowballStemmer
from langdetect import detect

# Clustering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

#config
from config import PARAMS
lang_dict = PARAMS.LANG_DICT
    
def random_articles(languages, articles_per_lang):
    """
    Extract random articles from Wikepedia.
    Returns a shuffled list of articles of different languages
    
    Arguments:
    * languages: a list of languages of the documents
    * articles_per_lang: number of articles per language
    
    """
    articles = []
    
    for language in languages:
        wikipedia.set_lang(language)
        titles = wikipedia.random(pages = articles_per_lang*2)
        count = 0
        article = ''
        for title in titles:
            try:
                article = wikipedia.page(title=title, auto_suggest='True', redirect=True).content
            except:
                pass
            articles.append(article)
            count += 1
            if count >= articles_per_lang:
                break
    
    random.shuffle(articles)
    return articles


def detect_language(articles, n_lang):
    '''
    Detects the language of the articles
    Returns a dictionnary of list of articles per language
    Argument:
    * articles: list of articles
    '''
    articles_dict = dict()
    for article in articles:
        try:
            lang = detect(article)
        except:
            pass
        if lang not in articles_dict.keys():
            articles_dict[lang] = []
        articles_dict[lang].append(article)

    # delete language keys with less than half of expected number of articles (probably incorrect language detection)
    for lang in list(articles_dict):
        if len(articles_dict[lang]) < len(articles)/(n_lang*4):
            articles_dict.pop(lang)
    
    return articles_dict


def process_article(article, lang):
    '''
    Process article by language
    Returns: list of tokens
    
    Parameters:
    * article: article to process (String)
    * lang: language
    
    '''
    stopwords = nltk.corpus.stopwords.words(lang_dict[lang])
    stemmer = SnowballStemmer(lang_dict[lang])
    
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word for sent in nltk.sent_tokenize(article) for word in nltk.word_tokenize(sent)]
    
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    filtered_tokens = []
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    
    # stem the words
    stems = [stemmer.stem(t) for t in filtered_tokens]
    
    # remove stopwords
    final_tokens = [token for token in stems if token not in stopwords]
    return final_tokens


def tfidf_features(articles):
    '''
    Get the tf-idf matrix for the articles
    Returns: tf_idf matrix
    
    Parameters:
    * articles: list of tokenized articles
    '''
    
    articles = [' '.join(article) for article in articles]
    tfidf_vectorizer = TfidfVectorizer(min_df=0.15, max_df=0.8, ngram_range=(1,3))
    tfidf_matrix = tfidf_vectorizer.fit_transform(articles)
    tfidf_labels = tfidf_vectorizer.get_feature_names()
    return tfidf_matrix, tfidf_labels


def optimal_clusters(articles, lang, max_nclusters=10):
    '''
    Find the optimal number of clusters by using the elbow method
    
    Parameters:
    * articles: list of Strings
    * max_nclusters: max number of cluster to try 
    '''
    
    # Calculate the intertia for each number of clusters
    results = []
    for i in range(2, max_nclusters+1):
        results.append(KMeans(n_clusters=i).fit(articles).inertia_)
    
    # Plot the results
    f, ax = plt.subplots(1, 1, figsize = (8,5))
    ax.plot(range(2, max_nclusters+1), results)
    ax.set_xlabel('NÂ° Clusters')
    ax.set_ylabel('Inertia')
    ax.set_title('Inertia by number of clusters for '+lang_dict[lang]+' language')
    plt.show()
    
def cluster_documents(articles, n_clusters=3):
    '''
    Perform a K-means clustering of the algorithms
    Returns: a list of cluster numbers
    
    Parameters:
    articles: list of Strings
    n_clusters: number of cluster to find 
    '''
    
    # Generate the clusters
    model = KMeans(n_clusters=n_clusters)
    clusters = model.fit(articles).labels_.tolist()
    
    return clusters

def plot_clusters(features, labels, lang):
    '''
    Plot the clusters generated by our Kmeans algorithm
    
    Parameters:
    * features: tfidf features of articles
    * labels: cluster labels for each article
    '''
    
    pca = PCA(n_components=2).fit_transform(features)
    
    f, ax = plt.subplots(1, 1, figsize = (10, 7))
    scatter = ax.scatter(pca[:, 0], pca[:, 1], c=labels)
    legend = ax.legend(*scatter.legend_elements(), loc="lower left", title="Classes")
    ax.add_artist(legend)
    ax.set_title('PCA Cluster Plot for '+lang_dict[lang]+' language')
    plt.show()

def top_keywords_per_cluster(features, cluster_labels, tfidf_labels, n_terms=10):
    '''
    Get top keywords per cluster
    Returns a list of lists of top keywords per cluster
    
    Parameters:
    * features: tfidf features of articles
    * cluster_labels: cluster labels for each article
    * tfidf_labels: tfidf feature names
    * n_terms: number of keywords per cluster
    '''
    
    df_keywords = pd.DataFrame(features)
    df_keywords['labels'] = cluster_labels
    keywords = []
    for index, row in df_keywords.groupby('labels').mean().iterrows():
        keywords.append([tfidf_labels[t] for t in np.argsort(row)[-n_terms:]])
    
    return keywords
